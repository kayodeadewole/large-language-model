{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We demonstrate the use of LangChain with OpenAI in this file\n",
    "#### To run the code here, you need to switch the Python kernel environment to where the required packages were installed or you start Jupyter-notebook from the environment. We demonstrated three operations here by using LangChain and OpenAI API.\n",
    "- Instruction Tuned LLM. Here, we simply demonstrate how to call OpenAI API with our prompt and get the response.\n",
    "- Instruction Tuned LLM advanced. Here, we demonstrate how to use template and customize prompt parameters using OpenAI Chat model\n",
    "- Dialog Tuned LLM. Here, we will interact with OpenAI Chatbot model at advanced level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(prompt=\"explain artificial intelligence in one sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the usage of Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demonstrating the usage of Chat Model\n",
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert in machine learning\"),\n",
    "    HumanMessage(content=\"Write a Python script to train a neural network on sample data\")\n",
    "    ]\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Prompt Templating. To create dynamic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demonstrating Prompt Templating. To create dynamic prompt\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template  = \"\"\"\n",
    "You are an expert in machine learning and you can build deep learning models. Explain the concept of {concept} in five lines.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['concept'], template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now format the prompt with the user input or data and inject it into the template\n",
    "llm(prompt.format(concept=\"regularization\")) #this can be changed to any user's input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the concept of Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets demonstrate the concept of Chains\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "#Run the chain by specifying the input variable\n",
    "print(chain.run(\"regularization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify mode chains. The second chain will take the output of the first chain and so on.\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me as a small kid\",\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We then combine the two chains into an overall chain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "#Run the chains specifying the input variable\n",
    "explanation = overall_chain.run(\"regularization\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the Embedding storing using vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets demonstrate the Embedding storing using vector database e.g PineCone. We split the text, convert it to embadding and store it in PineCone index.\n",
    "#PineCone index is simply the database name. The index must be the name of pinecone index.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the first element of the texts documents. i.e the corpus\n",
    "texts[0].page_content #the page content helps us with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets turn the texts i.e corpus to embedding. We can use OpenAI embedding to do this\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets search for the first document in our corpus i.e texts in the embedding vectors to know it works.\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "query_result #this will display the vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets store the vector representation (i.e vector embedding) inside the PineCone index (i.e db).\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "#Initialize pinecone\n",
    "pinecone.init(api_key=os.getenv['PINECONE_API_KEY'], environment=os.getenv(['PINECONE_ENV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-test\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do similarity_search on the PineCone index that we just created. We can search a specific document.\n",
    "query=\"What is important about regularization?\"\n",
    "result = search.similarity_search(query)\n",
    "result\n",
    "#If you do to your PineCone account, you will see everything inside the index. In vector database, index is called the database name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the concepts of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets demonstrate the concepts of Agents like ChatBot. Here, we will ask OpenAI ChatBot to solve a mathematical equation problem e.g quadratic\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We instantiate a python agent executor\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPL(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 6 * x**2 + 3*x -5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
